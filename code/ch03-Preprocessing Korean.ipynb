{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도 학습 기반 형태소 분석\n",
    "## KoNLPy 형태소 분석기 활용\n",
    "\n",
    "- Mecab (a.k.a 은전한닢)을 활용한 형태소 분석이 가장 대중적인 Tokenization 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아버지', '가', '방', '에', '들어가', '신다']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "tokenizer.morphs('아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `morphs` function을 이용하면 형태소 단위의 **tokenization**이 가능하고,\n",
    "- `pos` function을 이용하면 각 형태소의 **POS (Part-Of-Speech)** Tag를 형태소와 함께 출력 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('아버지', 'NNG'),\n",
       " ('가', 'JKS'),\n",
       " ('방', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('들어가', 'VV'),\n",
       " ('신다', 'EP+EC')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pos('아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교재에서는 KoNLPy에 내장되어 있는 다양한 형태소 분석기를 유연하게 사용하기 위해 `get_tokenizer` 함수를 정의하였으나,\n",
    "- `Mecab`을 사용할 수 있는 환경이라면 굳이 다른 형태소 분석기를 사용할 이유가 없는 것으로 알고 있음\n",
    "\n",
    "> +) **조건문**으로 인한 분기를 최대한 피하고 의식적으로 **Key access**를 지향하자!\n",
    ">\n",
    "> _github repository 내 [slide](https://github.com/nlp-kkmas/korean-embedding-study/blob/master/treasure/Clean%20code%20for%20AI%2C%20ML.pdf) 참고_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
    "\n",
    "def get_tokenizer(name):\n",
    "    key = {'komoran': Komoran(),\n",
    "           'okt': Okt(),\n",
    "           'mecab': Mecab(),\n",
    "           'hannanum': Hannanum(),\n",
    "           'kkma': Kkma()}\n",
    "    \n",
    "    if name not in key:\n",
    "        name = 'mecab'\n",
    "    \n",
    "    return key[name]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기정의한 `get_tokenizer` function을 이용해 형태소 분석기를 생성하는 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('komoran')\n",
    "print(tokenizer.morphs('아버지가방에들어가신다'))\n",
    "print(tokenizer.pos('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('bullshit')\n",
    "print(type(tokenizer))\n",
    "print(tokenizer.morphs('아버지가방에들어가신다'))\n",
    "print(tokenizer.pos('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> KoNLPy 내 형태소 분석기 간 비교는 다음 글을 참조하자 ! : [누가누가 잘하나! 대화체와 합이 잘 맞는 Tokenizer를 찾아보자!](https://blog.pingpong.us/tokenizer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khaiii 형태소 분석기 활용\n",
    "\n",
    "- MacOS, linux 환경에서는 카카오가 제작한 `Khaiii` 형태소 분석기를 활용 가능\n",
    "- Mecab 등과 달리 자연어 처리에 적용할 경우, **원문을 복원**하는 과정이 추가적으로 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "\n",
    "tokenizer = KhaiiiApi()\n",
    "data = tokenizer.analyze(\"아버지가방에들어가신다\")\n",
    "\n",
    "tokens = []\n",
    "clean_tokens = []\n",
    "\n",
    "for word in data:\n",
    "    tokens.extend([m.lex for m in word.morphs])\n",
    "    clean_tokens.extend([(m.lex, m.tag) for m in word.morphs])\n",
    "\n",
    "print(f'Tokens with morpheme information: {tokens}')\n",
    "print(f'Tokens without morpheme information: {clean_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 사전 추가\n",
    "\n",
    "- 진행하는 프로젝트에 따라 기학습된 규칙을 기반으로 형태소 분석을 수행하는 KoNLPy 형태소 분석기들에 **형태소로 분리되지 않아야 하는** 사용자 정의 단어를 추가해줄 필요가 있을 수 있음\n",
    "- 이럴 때 **사용자 사전**을 정의해 각 형태소 분석기에 추가해줄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가우스', '전자', '텔레비전', '정말', '좋', '네요']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad case:\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "tokenizer.morphs('가우스전자 텔레비전 정말 좋네요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표층형/0/0/0/품사태그/의미분류/종성유무/읽기/타입/첫번째품사/마지막품사/표현 ([link](https://kugancity.tistory.com/entry/mecab%EC%97%90-%EC%82%AC%EC%9A%A9%EC%9E%90%EC%82%AC%EC%A0%84%EA%B8%B0%EB%B6%84%EC%84%9D-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0))\n",
    "\n",
    "> 가우스전자,,,,NNP,**,T,가우스전자,**,**,**,**,**\n",
    "\n",
    "> bash preprocess.sh [mecab-user-dic](https://github.com/ratsgo/embedding/blob/master/preprocess.sh#L195) 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비지도 학습 기반 형태소 분석\n",
    "\n",
    "## soynlp\n",
    "- 형태소 분석, 품사 판별 등을 지원하는 한국어 자연어 처리 패키지\n",
    "- 하나의 문장/문서에서보다는 어느 정도 규모가 있으면서 동질적인 문서 집합에서 잘 작동\n",
    "- soynlp의 학습 지표\n",
    "    1. **Cohesion Probability**: 주어진 문자열이 유기적으로 연결돼 함께 자주 나타나는가?\n",
    "    2. **Branching Entropy**: 해당 단어 앞뒤로 다양한 조사, 어미 혹은 다른 단어가 등장하는가?\n",
    "- soynlp와 같은 비지도 학습 기반의 library는 corpus를 바탕으로 한 학습 이후 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soynlp 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.745 Gbse memory 0.541 Gb\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "\n",
    "corpus_fname = 'data/processed_ratings.txt'\n",
    "model_fname = 'data/soyword.model'\n",
    "\n",
    "sentences = [sent.strip() for sent in open(corpus_fname, 'r').readlines()]\n",
    "word_extractor = WordExtractor(min_frequency=100,\n",
    "                               min_cohesion_forward=0.05,\n",
    "                               min_right_branching_entropy=0.0)\n",
    "word_extractor.train(sentences)\n",
    "word_extractor.save(model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습된 soynlp 모델의 점수를 이용한 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 6130\n",
      "all branching entropies was computed # words = 123575\n",
      "all accessor variety was computed # words = 123575\n",
      "['애비는종이었다']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "model_fname = 'data/soyword.model'\n",
    "word_extractor = WordExtractor(min_frequency=100,\n",
    "                               min_cohesion_forward=0.05,\n",
    "                               min_right_branching_entropy=0.0)\n",
    "word_extractor.load(model_fname)\n",
    "\n",
    "scores = word_extractor.word_scores()\n",
    "scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}\n",
    "\n",
    "tokenizer = LTokenizer(scores=scores)\n",
    "tokens = tokenizer.tokenize(\"애비는 종이었다\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google SentencePiece\n",
    "- Google에서 공개한 BPE (Byte Pair Encoding) 기반의 Tokenizer\n",
    "- Corpus에서 가장 많이 등장한 문자열을 병합해 압축하여 사용\n",
    "- Goal: 원하는 Vocab size를 갖출 때 까지 반복적으로 고빈도 문자열을 병합해 Vocab에 추가 !\n",
    "    - 띄어쓰기를 기준으로 나눈 어절에 Sub-word가 포함되어 있는 경우 **최장 길이 일치**를 기준으로 Subword를 분리\n",
    "    - Subword 탐색을 반복한 후, Vocab에 없는 단어를 만나는 경우 **OOV** (Out Of Vocabulary) 처리\n",
    "- **BERT**를 비롯한 다양한 신모델들은 BPE 기반의 Tokenizer를 사용\n",
    "    - 사실상 **Transformer**의 등장 이후, 영어권에서는 **BPE Tokenization**이 **최선책**으로 등극"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece 학습 코드 (don't run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "train = '--input=data/processed_wiki_ko.txt --model_prefix=sentpiece --vocab_size=32000 --model_type=bpe --character_coverage=0.9995'\n",
    "spm.SentencePieceTrainer.Train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE 기반의 BERT Tokenizer (using. special tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5d1831bfd582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/bert.vocab'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'bert'"
     ]
    }
   ],
   "source": [
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "vocab_fname = 'data/bert.vocab'\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)\n",
    "\n",
    "print(tokenizer.tokenize('집에좀 가자'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 띄어쓰기 교정 모델 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soyspacing.countbase import CountSpace\n",
    "\n",
    "corpus_fname = 'data/processed_ratings.txt'\n",
    "model_fname = 'data/space-correct.model'\n",
    "\n",
    "model = CountSpace()\n",
    "model.train(corpus_fname)\n",
    "model.save_model(model_fname, json_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 띄어쓰기 교정 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('어릴때 보고 지금 다시봐도 재밌어요', [0, 0, 1, 0, 1, 0, 1, 0, None, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soyspacing.countbase import CountSpace\n",
    "\n",
    "model_fname = 'data/space-correct.model'\n",
    "model = CountSpace()\n",
    "model.load_model(model_fname, json_format=False)\n",
    "model.correct('어릴때보고 지금다시봐도 재밌어요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참조. MaxScoreTokenizer\n",
    "띄어쓰기가 제대로 지켜지지 않은 데이터라면, 문장의 띄어쓰기 기준으로 나뉘어진 단위가 L + [R] 구조라 가정할 수 없습니다. 하지만 사람은 띄어쓰기가 지켜지지 않은 문장에서 익숙한 단어부터 눈에 들어옵니다. 이 과정을 모델로 옮긴 MaxScoreTokenizer 역시 단어 점수를 이용합니다.\n",
    "\n",
    "```\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "scores = {'파스': 0.3, '파스타': 0.7, '좋아요': 0.2, '좋아':0.5}\n",
    "tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "\n",
    "print(tokenizer.tokenize('난파스타가좋아요'))\n",
    "# ['난', '파스타', '가', '좋아', '요']\n",
    "\n",
    "print(tokenizer.tokenize('난파스타가 좋아요', flatten=False))\n",
    "# [[('난', 0, 1, 0.0, 1), ('파스타', 1, 4, 0.7, 3),  ('가', 4, 5, 0.0, 1)],\n",
    "#  [('좋아', 0, 2, 0.5, 2), ('요', 2, 3, 0.0, 1)]]\n",
    "```\n",
    "\n",
    "MaxScoreTokenizer 역시 WordExtractor 의 결과를 이용하실 때에는 위의 예시처럼 적절히 scores 를 만들어 사용합니다. 이미 알려진 단어 사전이 있다면 이 단어들은 다른 어떤 단어보다도 더 큰 점수를 부여하면 그 단어는 토크나이저가 하나의 단어로 잘라냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 구어체에 띄어쓰기를 적용하고 싶을 경우, [Ping-pong's chatspace](https://github.com/pingpong-ai/chatspace) 참조 !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

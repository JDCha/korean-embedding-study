{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from torch.autograd import Variable\n",
    "from .modules.elmo import ElmobiLm\n",
    "from .modules.token_embedder import ConvTokenEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_batch(x, word2id, char2id, config, oov='<oov>', pad='<pad>', sort=True):\n",
    "    \"\"\"\n",
    "    Create one batch of input.\n",
    "    :param x: List[List[str]]\n",
    "    :param word2id: Dict | None\n",
    "    :param char2id: Dict | None\n",
    "    :param config: Dict\n",
    "    :param oov: str, the form of OOV token.\n",
    "    :param pad: str, the form of padding token.\n",
    "    :param sort: bool, specify whether sorting the sentences by their lengths.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_size = len(x)\n",
    "    # lst represents the order of sentences\n",
    "    lst = list(range(batch_size))\n",
    "    if sort:\n",
    "        lst.sort(key=lambda l: -len(x[l]))\n",
    "\n",
    "    # shuffle the sentences by\n",
    "    x = [x[i] for i in lst]\n",
    "    lens = [len(x[i]) for i in lst]\n",
    "    max_len = max(lens)\n",
    "\n",
    "    # get a batch of word id whose size is (batch x max_len)\n",
    "    if word2id is not None:\n",
    "        oov_id, pad_id = word2id.get(oov, None), word2id.get(pad, None)\n",
    "        assert oov_id is not None and pad_id is not None\n",
    "        batch_w = torch.LongTensor(batch_size, max_len).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_w[i][j] = word2id.get(x_ij, oov_id)\n",
    "    else:\n",
    "        batch_w = None\n",
    "\n",
    "    # get a batch of character id whose size is (batch x max_len x max_chars)\n",
    "    if char2id is not None:\n",
    "        bow_id, eow_id, oov_id, pad_id = [char2id.get(key, None) for key in ('<eow>', '<bow>', oov, pad)]\n",
    "\n",
    "        assert bow_id is not None and eow_id is not None and oov_id is not None and pad_id is not None\n",
    "\n",
    "        max_chars = config['token_embedder']['max_characters_per_token']\n",
    "        assert max([len(w) for i in lst for w in x[i]]) + 2 <= max_chars\n",
    "\n",
    "        batch_c = torch.LongTensor(batch_size, max_len, max_chars).fill_(pad_id)\n",
    "\n",
    "        for i, x_i in enumerate(x):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_c[i][j][0] = bow_id\n",
    "                if x_ij == '<bos>' or x_ij == '<eos>':\n",
    "                    batch_c[i][j][1] = char2id.get(x_ij)\n",
    "                    batch_c[i][j][2] = eow_id\n",
    "                else:\n",
    "                    for k, c in enumerate(x_ij):\n",
    "                        batch_c[i][j][k + 1] = char2id.get(c, oov_id)\n",
    "                    batch_c[i][j][len(x_ij) + 1] = eow_id\n",
    "    else:\n",
    "        batch_c = None\n",
    "\n",
    "    # mask[0] is the matrix (batch x max_len) indicating whether\n",
    "    # there is an id is valid (not a padding) in this batch.\n",
    "    # mask[1] stores the flattened ids indicating whether there is a valid\n",
    "    # previous token\n",
    "    # mask[2] stores the flattened ids indicating whether there is a valid\n",
    "    # next token\n",
    "    masks = [torch.LongTensor(batch_size, max_len).fill_(0), [], []]\n",
    "\n",
    "    for i, x_i in enumerate(x):\n",
    "        for j in range(len(x_i)):\n",
    "            masks[0][i][j] = 1\n",
    "            if j + 1 < len(x_i):\n",
    "                masks[1].append(i * max_len + j)\n",
    "            if j > 0:\n",
    "                masks[2].append(i * max_len + j)\n",
    "\n",
    "    assert len(masks[1]) <= batch_size * max_len\n",
    "    assert len(masks[2]) <= batch_size * max_len\n",
    "\n",
    "    masks[1] = torch.LongTensor(masks[1])\n",
    "    masks[2] = torch.LongTensor(masks[2])\n",
    "\n",
    "    return batch_w, batch_c, lens, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training examples and create mini-batches\n",
    "def create_batches(x, batch_size, word2id, char2id, config, perm=None, shuffle=True, sort=True, text=None):\n",
    "    \"\"\"\n",
    "    :param x: List[List[str]]\n",
    "    :param batch_size:\n",
    "    :param word2id:\n",
    "    :param char2id:\n",
    "    :param config:\n",
    "    :param perm:\n",
    "    :param shuffle:\n",
    "    :param sort:\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lst = perm or list(range(len(x)))\n",
    "    if shuffle:\n",
    "        random.shuffle(lst)\n",
    "\n",
    "    if sort:\n",
    "        lst.sort(key=lambda l: -len(x[l]))\n",
    "\n",
    "    x = [x[i] for i in lst]\n",
    "    if text is not None:\n",
    "        text = [text[i] for i in lst]\n",
    "\n",
    "    sum_len = 0.0\n",
    "    batches_w, batches_c, batches_lens, batches_masks, batches_text = [], [], [], [], []\n",
    "    size = batch_size\n",
    "    nbatch = (len(x) - 1) // size + 1\n",
    "    for i in range(nbatch):\n",
    "        start_id, end_id = i * size, (i + 1) * size\n",
    "        bw, bc, blens, bmasks = create_one_batch(x[start_id: end_id], word2id, char2id, config, sort=sort)\n",
    "        sum_len += sum(blens)\n",
    "        batches_w.append(bw)\n",
    "        batches_c.append(bc)\n",
    "        batches_lens.append(blens)\n",
    "        batches_masks.append(bmasks)\n",
    "        if text is not None:\n",
    "            batches_text.append(text[start_id: end_id])\n",
    "\n",
    "    if sort:\n",
    "        perm = list(range(nbatch))\n",
    "        random.shuffle(perm)\n",
    "        batches_w = [batches_w[i] for i in perm]\n",
    "        batches_c = [batches_c[i] for i in perm]\n",
    "        batches_lens = [batches_lens[i] for i in perm]\n",
    "        batches_masks = [batches_masks[i] for i in perm]\n",
    "        if text is not None:\n",
    "            batches_text = [batches_text[i] for i in perm]\n",
    "\n",
    "    logging.info(\"{} batches, avg len: {:.1f}\".format(nbatch, sum_len / len(x)))\n",
    "    if text is not None:\n",
    "        return batches_w, batches_c, batches_lens, batches_masks, batches_text\n",
    "    return batches_w, batches_c, batches_lens, batches_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config, word_emb_layer, char_emb_layer, use_cuda=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.config = config\n",
    "\n",
    "        self.token_embedder = ConvTokenEmbedder(\n",
    "          config, word_emb_layer, char_emb_layer, use_cuda)\n",
    "        self.encoder = ElmobiLm(config, use_cuda)\n",
    "        self.output_dim = config['encoder']['projection_dim']\n",
    "\n",
    "    def forward(self, word_inp, chars_package, mask_package):\n",
    "        \"\"\"\n",
    "        :param word_inp:\n",
    "        :param chars_package:\n",
    "        :param mask_package:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        token_embedding = self.token_embedder(word_inp, chars_package, (mask_package[0].size(0), mask_package[0].size(1)))\n",
    "        mask = Variable(mask_package[0]).cuda() if self.use_cuda else Variable(mask_package[0])\n",
    "        encoder_output = self.encoder(token_embedding, mask)\n",
    "        sz = encoder_output.size()\n",
    "        token_embedding = torch.cat(\n",
    "          [token_embedding, token_embedding], dim=2).view(1, sz[1], sz[2], sz[3])\n",
    "        encoder_output = torch.cat(\n",
    "          [token_embedding, encoder_output], dim=0)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.token_embedder.load_state_dict(torch.load(os.path.join(path, 'token_embedder.pkl'),\n",
    "                                                       map_location=lambda storage, loc: storage))\n",
    "        self.encoder.load_state_dict(torch.load(os.path.join(path, 'encoder.pkl'),\n",
    "                                                map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from .modules.embedding_layer import EmbeddingLayer\n",
    "import collections\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path, max_chars=None):\n",
    "    \"\"\"\n",
    "    read raw text file. The format of the input is like, one sentence per line\n",
    "    words are separated by '\\t'\n",
    "    :param path:\n",
    "    :param max_chars: int, the number of maximum characters in a word, this\n",
    "    parameter is used when the model is configured with CNN word encoder.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    textset = []\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin.read().strip().split('\\n'):\n",
    "            data = ['<bos>']\n",
    "            text = []\n",
    "            for token in line.split('\\t'):\n",
    "                text.append(token)\n",
    "                if max_chars is not None and len(token) + 2 > max_chars:\n",
    "                    token = token[:max_chars - 2]\n",
    "                data.append(token)\n",
    "            data.append('<eos>')\n",
    "            dataset.append(data)\n",
    "            textset.append(text)\n",
    "    return dataset, textset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_main():\n",
    "    # Configurations\n",
    "    cmd = argparse.ArgumentParser('The testing components of')\n",
    "    cmd.add_argument('--gpu', default=-1, type=int, help='use id of gpu, -1 if cpu.')\n",
    "    cmd.add_argument('--input_format', default='plain', choices=('plain', 'conll', 'conll_char', 'conll_char_vi'),\n",
    "                   help='the input format.')\n",
    "    cmd.add_argument(\"--input\", help=\"the path to the raw text file.\")\n",
    "    cmd.add_argument(\"--output_format\", default='hdf5', help='the output format. Supported format includes (hdf5, txt).'\n",
    "                                                           ' Use comma to separate the format identifiers,'\n",
    "                                                           ' like \\'--output_format=hdf5,plain\\'')\n",
    "    cmd.add_argument(\"--output_prefix\", help='the prefix of the output file. The output file is in the format of '\n",
    "                                           '<output_prefix>.<output_layer>.<output_format>')\n",
    "    cmd.add_argument(\"--output_layer\", help='the target layer to output. 0 for the word encoder, 1 for the first LSTM '\n",
    "                                          'hidden layer, 2 for the second LSTM hidden layer, -1 for an average'\n",
    "                                          'of 3 layers.')\n",
    "    cmd.add_argument(\"--model\", required=True, help=\"the path to the model.\")\n",
    "    cmd.add_argument(\"--batch_size\", \"--batch\", type=int, default=1, help='the batch size.')\n",
    "    args = cmd.parse_args(sys.argv[2:])\n",
    "\n",
    "    if args.gpu >= 0:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "    use_cuda = args.gpu >= 0 and torch.cuda.is_available()\n",
    "    # load the model configurations\n",
    "    args2 = dict2namedtuple(json.load(codecs.open(os.path.join(args.model, 'config.json'), 'r', encoding='utf-8')))\n",
    "\n",
    "    with open(os.path.join(args.model, args2.config_path), 'r') as fin:\n",
    "        config = json.load(fin)\n",
    "\n",
    "    # For the model trained with character-based word encoder.\n",
    "    if config['token_embedder']['char_dim'] > 0:\n",
    "        char_lexicon = {}\n",
    "        with codecs.open(os.path.join(args.model, 'char.dic'), 'r', encoding='utf-8') as fpi:\n",
    "            for line in fpi:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                if len(tokens) == 1:\n",
    "                    tokens.insert(0, '\\u3000')\n",
    "                token, i = tokens\n",
    "                char_lexicon[token] = int(i)\n",
    "        char_emb_layer = EmbeddingLayer(config['token_embedder']['char_dim'], char_lexicon, fix_emb=False, embs=None)\n",
    "        logging.info('char embedding size: ' + str(len(char_emb_layer.word2id)))\n",
    "    else:\n",
    "        char_lexicon = None\n",
    "        char_emb_layer = None\n",
    "\n",
    "    # For the model trained with word form word encoder.\n",
    "    if config['token_embedder']['word_dim'] > 0:\n",
    "        word_lexicon = {}\n",
    "        with codecs.open(os.path.join(args.model, 'word.dic'), 'r', encoding='utf-8') as fpi:\n",
    "            for line in fpi:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                if len(tokens) == 1:\n",
    "                    tokens.insert(0, '\\u3000')\n",
    "                token, i = tokens\n",
    "                word_lexicon[token] = int(i)\n",
    "        word_emb_layer = EmbeddingLayer(config['token_embedder']['word_dim'], word_lexicon, fix_emb=False, embs=None)\n",
    "        logging.info('word embedding size: ' + str(len(word_emb_layer.word2id)))\n",
    "    else:\n",
    "        word_lexicon = None\n",
    "        word_emb_layer = None\n",
    "\n",
    "    # instantiate the model\n",
    "    model = Model(config, word_emb_layer, char_emb_layer, use_cuda)\n",
    "\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    logging.info(str(model))\n",
    "    model.load_model(args.model)\n",
    "\n",
    "    # read test data according to input format\n",
    "    test, text = read_corpus(args.input, config['token_embedder']['max_characters_per_token'])\n",
    "\n",
    "    # create test batches from the input data.\n",
    "    test_w, test_c, test_lens, test_masks, test_text = create_batches(\n",
    "      test, args.batch_size, word_lexicon, char_lexicon, config, text=text)\n",
    "\n",
    "    # configure the model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    sent_set = set()\n",
    "    cnt = 0\n",
    "\n",
    "    output_formats = args.output_format.split(',')\n",
    "    output_layers = map(int, args.output_layer.split(','))\n",
    "\n",
    "    handlers = {}\n",
    "    for output_format in output_formats:\n",
    "        if output_format not in ('hdf5', 'txt'):\n",
    "            print('Unknown output_format: {0}'.format(output_format))\n",
    "            continue\n",
    "        for output_layer in output_layers:\n",
    "            filename = '{0}.ly{1}.{2}'.format(args.output_prefix, output_layer, output_format)\n",
    "            handlers[output_format, output_layer] = \\\n",
    "              h5py.File(filename, 'w') if output_format == 'hdf5' else open(filename, 'w')\n",
    "\n",
    "    for w, c, lens, masks, texts in zip(test_w, test_c, test_lens, test_masks, test_text):\n",
    "        output = model.forward(w, c, masks)\n",
    "        for i, text in enumerate(texts):\n",
    "            sent = '\\t'.join(text)\n",
    "            sent = sent.replace('.', '$period$')\n",
    "            sent = sent.replace('/', '$backslash$')\n",
    "            if sent in sent_set:\n",
    "                continue\n",
    "            sent_set.add(sent)\n",
    "            data = output[:, i, 1:lens[i]-1, :].data\n",
    "            if use_cuda:\n",
    "                data = data.cpu()\n",
    "            data = data.numpy()\n",
    "\n",
    "            for (output_format, output_layer) in handlers:\n",
    "                fout = handlers[output_format, output_layer]\n",
    "                if output_layer == -1:\n",
    "                    payload = np.average(data, axis=0)\n",
    "                else:\n",
    "                    payload = data[output_layer]\n",
    "                if output_format == 'hdf5':\n",
    "                    fout.create_dataset(sent, payload.shape, dtype='float32', data=payload)\n",
    "                else:\n",
    "                    for word, row in zip(text, payload):\n",
    "                        print('{0}\\t{1}'.format(word, '\\t'.join(['{0:.8f}'.format(elem) for elem in row])), file=fout)\n",
    "                    print('', file=fout)\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                logging.info('Finished {0} sentences.'.format(cnt))\n",
    "    for _, handler in handlers.items():\n",
    "        handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

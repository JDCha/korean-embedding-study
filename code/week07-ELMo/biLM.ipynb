{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo Basic Implementation\n",
    "\n",
    "- https://github.com/HIT-SCIR/ELMoForManyLangs\n",
    "\n",
    "모듈 구조\n",
    "\n",
    "- `embedding_layer.py` 워드 및 문자 임베딩\n",
    "- `token_embedder.py` 문자 단위 CNN 네트워크 (highway 네트워크 포함)\n",
    "- `elmo.py` 양방향 LSTM 네트워크\n",
    "- `classify_layer.py` softmax 레이어\n",
    "\n",
    "학습 방법\n",
    "\n",
    "- `biLM.ipynb` 모델 pre-train \n",
    "- `frontend.ipynb` 학습된 pre-train 모델을 토대로 임베딩 벡터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "import os\n",
    "import errno\n",
    "import sys\n",
    "import codecs\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from .modules.elmo import ElmobiLm\n",
    "from .modules.token_embedder import ConvTokenEmbedder\n",
    "from .modules.embedding_layer import EmbeddingLayer\n",
    "from .modules.classify_layer import SoftmaxLayer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_npz(path):\n",
    "    data = np.load(path)\n",
    "    return [str(w) for w in data['words']], data['vals']\n",
    "\n",
    "\n",
    "def load_embedding_txt(path):\n",
    "    words = []\n",
    "    vals = []\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as fin:\n",
    "        fin.readline()\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                words.append(parts[0])\n",
    "                vals += [float(x) for x in parts[1:]]  # equal to append\n",
    "    return words, np.asarray(vals).reshape(len(words), -1)  # reshape\n",
    "\n",
    "\n",
    "def load_embedding(path):\n",
    "    if path.endswith(\".npz\"):\n",
    "        return load_embedding_npz(path)\n",
    "    else:\n",
    "        return load_embedding_txt(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2namedtuple(dic):\n",
    "    return collections.namedtuple('Namespace', dic.keys())(**dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(data, valid_size):\n",
    "    valid_size = min(valid_size, len(data) // 10)\n",
    "    random.shuffle(data)\n",
    "    return data[valid_size:], data[:valid_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_sentence(sentence, max_sent_len):\n",
    "    \"\"\"\n",
    "    For example, for a sentence with 70 words, supposing the the `max_sent_len'\n",
    "    is 30, break it into 3 sentences.\n",
    "    :param sentence: list[str] the sentence\n",
    "    :param max_sent_len:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    cur = 0\n",
    "    length = len(sentence)\n",
    "    while cur < length:\n",
    "        if cur + max_sent_len + 5 >= length:\n",
    "            ret.append(sentence[cur: length])\n",
    "            break\n",
    "        ret.append(sentence[cur: min(length, cur + max_sent_len)])\n",
    "        cur += max_sent_len\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path, max_chars=None, max_sent_len=20):\n",
    "    \"\"\"\n",
    "    read raw text file\n",
    "    :param path: str\n",
    "    :param max_chars: int\n",
    "    :param max_sent_len: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            data.append('<bos>')\n",
    "            for token in line.strip().split():\n",
    "                if max_chars is not None and len(token) + 2 > max_chars:\n",
    "                    token = token[:max_chars - 2]\n",
    "                data.append(token)\n",
    "            data.append('<eos>')\n",
    "    dataset = break_sentence(data, max_sent_len)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_batch(x, word2id, char2id, config, oov='<oov>', pad='<pad>', sort=True):\n",
    "    \"\"\"\n",
    "    :param x:\n",
    "    :param word2id: dict\n",
    "    :param char2id: dict\n",
    "    :param config:\n",
    "    :param oov:\n",
    "    :param pad:\n",
    "    :param sort:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_size = len(x)\n",
    "    lst = list(range(batch_size))\n",
    "    if sort:\n",
    "        lst.sort(key=lambda l: -len(x[l]))\n",
    "\n",
    "    x = [x[i] for i in lst]\n",
    "    lens = [len(x[i]) for i in lst]\n",
    "    max_len = max(lens)\n",
    "\n",
    "    if word2id is not None:\n",
    "        oov_id, pad_id = word2id.get(oov, None), word2id.get(pad, None)\n",
    "        assert oov_id is not None and pad_id is not None\n",
    "        batch_w = torch.LongTensor(batch_size, max_len).fill_(pad_id)\n",
    "        for i, x_i in enumerate(x):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_w[i][j] = word2id.get(x_ij, oov_id)\n",
    "    else:\n",
    "        batch_w = None\n",
    "\n",
    "    if char2id is not None:\n",
    "        bow_id, eow_id, oov_id, pad_id = char2id.get('<eow>', None), char2id.get('<bow>', None), char2id.get(oov, None), char2id.get(pad, None)\n",
    "\n",
    "        assert bow_id is not None and eow_id is not None and oov_id is not None and pad_id is not None\n",
    "\n",
    "        max_chars = config['token_embedder']['max_characters_per_token']\n",
    "        assert max([len(w) for i in lst for w in x[i]]) + 2 <= max_chars\n",
    "\n",
    "        batch_c = torch.LongTensor(batch_size, max_len, max_chars).fill_(pad_id)\n",
    "\n",
    "        for i, x_i in enumerate(x):\n",
    "            for j, x_ij in enumerate(x_i):\n",
    "                batch_c[i][j][0] = bow_id\n",
    "                if x_ij == '<bos>' or x_ij == '<eos>':\n",
    "                    batch_c[i][j][1] = char2id.get(x_ij)\n",
    "                    batch_c[i][j][2] = eow_id\n",
    "                else:\n",
    "                    for k, c in enumerate(x_ij):\n",
    "                        batch_c[i][j][k + 1] = char2id.get(c, oov_id)\n",
    "                    batch_c[i][j][len(x_ij) + 1] = eow_id\n",
    "    else:\n",
    "        batch_c = None\n",
    "\n",
    "    masks = [torch.LongTensor(batch_size, max_len).fill_(0), [], []]\n",
    "\n",
    "    for i, x_i in enumerate(x):\n",
    "        for j in range(len(x_i)):\n",
    "            masks[0][i][j] = 1\n",
    "                if j + 1 < len(x_i):\n",
    "                    masks[1].append(i * max_len + j)\n",
    "                if j > 0: \n",
    "                    masks[2].append(i * max_len + j)\n",
    "\n",
    "    assert len(masks[1]) <= batch_size * max_len\n",
    "    assert len(masks[2]) <= batch_size * max_len\n",
    "\n",
    "    masks[1] = torch.LongTensor(masks[1])\n",
    "    masks[2] = torch.LongTensor(masks[2])\n",
    "\n",
    "    return batch_w, batch_c, lens, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training examples and create mini-batches\n",
    "def create_batches(x, batch_size, word2id, char2id, config, perm=None, shuffle=True, sort=True, use_cuda=False):\n",
    "    \"\"\"\n",
    "    :param x:\n",
    "    :param batch_size:\n",
    "    :param word2id:\n",
    "    :param char2id:\n",
    "    :param config:\n",
    "    :param perm:\n",
    "    :param shuffle:\n",
    "    :param sort:\n",
    "    :param use_cuda:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lst = perm or list(range(len(x)))\n",
    "    if shuffle:\n",
    "        random.shuffle(lst)\n",
    "\n",
    "    if sort:\n",
    "        lst.sort(key=lambda l: -len(x[l]))\n",
    "\n",
    "    x = [x[i] for i in lst]\n",
    "\n",
    "    sum_len = 0.0\n",
    "    batches_w, batches_c, batches_lens, batches_masks = [], [], [], []\n",
    "    size = batch_size\n",
    "    nbatch = (len(x) - 1) // size + 1\n",
    "    for i in range(nbatch):\n",
    "        start_id, end_id = i * size, (i + 1) * size\n",
    "        bw, bc, blens, bmasks = create_one_batch(x[start_id: end_id], word2id, char2id, config, sort=sort)\n",
    "        sum_len += sum(blens)\n",
    "        batches_w.append(bw)\n",
    "        batches_c.append(bc)\n",
    "        batches_lens.append(blens)\n",
    "        batches_masks.append(bmasks)\n",
    "\n",
    "    if sort:\n",
    "        perm = list(range(nbatch))\n",
    "        random.shuffle(perm)\n",
    "        batches_w = [batches_w[i] for i in perm]\n",
    "        batches_c = [batches_c[i] for i in perm]\n",
    "        batches_lens = [batches_lens[i] for i in perm]\n",
    "        batches_masks = [batches_masks[i] for i in perm]\n",
    "\n",
    "    logging.info(\"{} batches, avg len: {:.1f}\".format(nbatch, sum_len / len(x)))\n",
    "    return batches_w, batches_c, batches_lens, batches_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config, word_emb_layer, char_emb_layer, n_class, use_cuda=False):\n",
    "        super(Model, self).__init__() \n",
    "        self.use_cuda = use_cuda\n",
    "        self.config = config\n",
    "\n",
    "        self.token_embedder = ConvTokenEmbedder(config, word_emb_layer, char_emb_layer, use_cuda)\n",
    "        self.encoder = ElmobiLm(config, use_cuda)\n",
    "        self.output_dim = config['encoder']['projection_dim']\n",
    "        self.classify_layer = SoftmaxLayer(self.output_dim, n_class)\n",
    "\n",
    "    def forward(self, word_inp, chars_inp, mask_package):\n",
    "        \"\"\"\n",
    "        :param word_inp:\n",
    "        :param chars_inp:\n",
    "        :param mask_package: Tuple[]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        token_embedding = self.token_embedder(word_inp, chars_inp, (mask_package[0].size(0), mask_package[0].size(1)))\n",
    "        token_embedding = F.dropout(token_embedding, self.config['dropout'], self.training)\n",
    "\n",
    "        mask = Variable(mask_package[0].cuda()).cuda() if self.use_cuda else Variable(mask_package[0])\n",
    "        encoder_output = self.encoder(token_embedding, mask)\n",
    "        encoder_output = encoder_output[1]\n",
    "        # [batch_size, len, hidden_size]\n",
    "\n",
    "        encoder_output = F.dropout(encoder_output, self.config['dropout'], self.training)\n",
    "        forward, backward = encoder_output.split(self.output_dim, 2)\n",
    "\n",
    "        word_inp = Variable(word_inp)\n",
    "        if self.use_cuda:\n",
    "            word_inp = word_inp.cuda()\n",
    "\n",
    "        mask1 = Variable(mask_package[1].cuda()).cuda() if self.use_cuda else Variable(mask_package[1])\n",
    "        mask2 = Variable(mask_package[2].cuda()).cuda() if self.use_cuda else Variable(mask_package[2])\n",
    "\n",
    "        forward_x = forward.contiguous().view(-1, self.output_dim).index_select(0, mask1)\n",
    "        forward_y = word_inp.contiguous().view(-1).index_select(0, mask2)\n",
    "\n",
    "        backward_x = backward.contiguous().view(-1, self.output_dim).index_select(0, mask2)\n",
    "        backward_y = word_inp.contiguous().view(-1).index_select(0, mask1)\n",
    "\n",
    "        return self.classify_layer(forward_x, forward_y), self.classify_layer(backward_x, backward_y)\n",
    "\n",
    "    def save_model(self, path, save_classify_layer):\n",
    "        torch.save(self.token_embedder.state_dict(), os.path.join(path, 'token_embedder.pkl'))    \n",
    "        torch.save(self.encoder.state_dict(), os.path.join(path, 'encoder.pkl'))\n",
    "        if save_classify_layer:\n",
    "            torch.save(self.classify_layer.state_dict(), os.path.join(path, 'classifier.pkl'))\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.token_embedder.load_state_dict(torch.load(os.path.join(path, 'token_embedder.pkl')))\n",
    "        self.encoder.load_state_dict(torch.load(os.path.join(path, 'encoder.pkl')))\n",
    "        self.classify_layer.load_state_dict(torch.load(os.path.join(path, 'classifier.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, valid):\n",
    "    model.eval()\n",
    "    total_loss, total_tag = 0.0, 0\n",
    "    valid_w, valid_c, valid_lens, valid_masks = valid\n",
    "    for w, c, lens, masks in zip(valid_w, valid_c, valid_lens, valid_masks):\n",
    "        loss_forward, loss_backward = model.forward(w, c, masks)\n",
    "        total_loss += loss_forward.data[0]\n",
    "        n_tags = sum(lens)\n",
    "        total_tag += n_tags\n",
    "    model.train()\n",
    "    return np.exp(total_loss / total_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, opt, model, optimizer,\n",
    "                train, valid, test, best_train, best_valid, test_result):\n",
    "    \"\"\"\n",
    "    Training model for one epoch\n",
    "    :param epoch:\n",
    "    :param opt:\n",
    "    :param model:\n",
    "    :param optimizer:\n",
    "    :param train:\n",
    "    :param best_train:\n",
    "    :param valid:\n",
    "    :param best_valid:\n",
    "    :param test:\n",
    "    :param test_result:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_tag = 0.0, 0\n",
    "    cnt = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_w, train_c, train_lens, train_masks = train\n",
    "\n",
    "    lst = list(range(len(train_w)))\n",
    "    random.shuffle(lst)\n",
    "\n",
    "    train_w = [train_w[l] for l in lst]\n",
    "    train_c = [train_c[l] for l in lst]\n",
    "    train_lens = [train_lens[l] for l in lst]\n",
    "    train_masks = [train_masks[l] for l in lst]\n",
    "\n",
    "    for w, c, lens, masks in zip(train_w, train_c, train_lens, train_masks):\n",
    "        cnt += 1\n",
    "        model.zero_grad()\n",
    "        loss_forward, loss_backward = model.forward(w, c, masks)\n",
    "\n",
    "        loss = (loss_forward + loss_backward) / 2.0\n",
    "        total_loss += loss_forward.data[0]\n",
    "        n_tags = sum(lens)\n",
    "        total_tag += n_tags\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), opt.clip_grad)\n",
    "        optimizer.step()\n",
    "        if cnt * opt.batch_size % 1024 == 0:\n",
    "            logging.info(\"Epoch={} iter={} lr={:.6f} train_ppl={:.6f} time={:.2f}s\".format(\n",
    "              epoch, cnt, optimizer.param_groups[0]['lr'],\n",
    "              np.exp(total_loss / total_tag), time.time() - start_time\n",
    "            ))\n",
    "            start_time = time.time()\n",
    "\n",
    "        if cnt % opt.eval_steps == 0 or cnt % len(train_w) == 0:\n",
    "            if valid is None:\n",
    "                train_ppl = np.exp(total_loss / total_tag)\n",
    "                logging.info(\"Epoch={} iter={} lr={:.6f} train_ppl={:.6f}\".format(\n",
    "                  epoch, cnt, optimizer.param_groups[0]['lr'], train_ppl))\n",
    "                if train_ppl < best_train:\n",
    "                    best_train = train_ppl\n",
    "                    logging.info(\"New record achieved on training dataset!\")\n",
    "                    model.save_model(opt.model, opt.save_classify_layer)      \n",
    "            else:\n",
    "                valid_ppl = eval_model(model, valid)\n",
    "                logging.info(\"Epoch={} iter={} lr={:.6f} valid_ppl={:.6f}\".format(\n",
    "                  epoch, cnt, optimizer.param_groups[0]['lr'], valid_ppl))\n",
    "\n",
    "                if valid_ppl < best_valid:\n",
    "                    model.save_model(opt.model, opt.save_classify_layer)\n",
    "                    best_valid = valid_ppl\n",
    "                    logging.info(\"New record achieved!\")\n",
    "\n",
    "                    if test is not None:\n",
    "                        test_result = eval_model(model, test)\n",
    "                        logging.info(\"Epoch={} iter={} lr={:.6f} test_ppl={:.6f}\".format(\n",
    "                          epoch, cnt, optimizer.param_groups[0]['lr'], test_result))\n",
    "    return best_train, best_valid, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truncated_vocab(dataset, min_count):\n",
    "    \"\"\"\n",
    "    :param dataset:\n",
    "    :param min_count: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sentence in dataset:\n",
    "        word_count.update(sentence)\n",
    "\n",
    "    word_count = list(word_count.items())\n",
    "    word_count.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    i = 0\n",
    "    for word, count in word_count:\n",
    "        if count < min_count:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "    logging.info('Truncated word count: {0}.'.format(sum([count for word, count in word_count[i:]])))\n",
    "    logging.info('Original vocabulary size: {0}.'.format(len(word_count)))\n",
    "    return word_count[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    cmd = argparse.ArgumentParser(sys.argv[0], conflict_handler='resolve')\n",
    "    cmd.add_argument('--seed', default=1, type=int, help='The random seed.')\n",
    "    cmd.add_argument('--gpu', default=-1, type=int, help='Use id of gpu, -1 if cpu.')\n",
    "\n",
    "    cmd.add_argument('--train_path', required=True, help='The path to the training file.')\n",
    "    cmd.add_argument('--valid_path', help='The path to the development file.')\n",
    "    cmd.add_argument('--test_path', help='The path to the testing file.')\n",
    "\n",
    "    cmd.add_argument('--config_path', required=True, help='the path to the config file.')\n",
    "    cmd.add_argument(\"--word_embedding\", help=\"The path to word vectors.\")\n",
    "\n",
    "    cmd.add_argument('--optimizer', default='sgd', choices=['sgd', 'adam', 'adagrad'],\n",
    "                   help='the type of optimizer: valid options=[sgd, adam, adagrad]')\n",
    "    cmd.add_argument(\"--lr\", type=float, default=0.01, help='the learning rate.')\n",
    "    cmd.add_argument(\"--lr_decay\", type=float, default=0, help='the learning rate decay.')\n",
    "\n",
    "    cmd.add_argument(\"--model\", required=True, help=\"path to save model\")\n",
    "\n",
    "    cmd.add_argument(\"--batch_size\", \"--batch\", type=int, default=32, help='the batch size.')\n",
    "    cmd.add_argument(\"--max_epoch\", type=int, default=100, help='the maximum number of iteration.')\n",
    "\n",
    "    cmd.add_argument(\"--clip_grad\", type=float, default=5, help='the tense of clipped grad.')\n",
    "\n",
    "    cmd.add_argument('--max_sent_len', type=int, default=20, help='maximum sentence length.')\n",
    "\n",
    "    cmd.add_argument('--min_count', type=int, default=5, help='minimum word count.')\n",
    "\n",
    "    cmd.add_argument('--max_vocab_size', type=int, default=150000, help='maximum vocabulary size.')\n",
    "\n",
    "    cmd.add_argument('--save_classify_layer', default=False, action='store_true',\n",
    "                   help=\"whether to save the classify layer\")\n",
    "\n",
    "    cmd.add_argument('--valid_size', type=int, default=0, help=\"size of validation dataset when there's no valid.\")\n",
    "    cmd.add_argument('--eval_steps', required=False, type=int, help='report every xx batches.')\n",
    "\n",
    "    opt = cmd.parse_args(sys.argv[2:])\n",
    "\n",
    "    with open(opt.config_path, 'r') as fin:\n",
    "        config = json.load(fin)\n",
    "\n",
    "    # Dump configurations\n",
    "    print(opt)\n",
    "    print(config)\n",
    "\n",
    "    # set seed.\n",
    "    torch.manual_seed(opt.seed)\n",
    "    random.seed(opt.seed)\n",
    "    if opt.gpu >= 0:\n",
    "        torch.cuda.set_device(opt.gpu)\n",
    "        if opt.seed > 0:\n",
    "            torch.cuda.manual_seed(opt.seed)\n",
    "\n",
    "    use_cuda = opt.gpu >= 0 and torch.cuda.is_available()\n",
    "\n",
    "    token_embedder_max_chars = config['token_embedder'].get('max_characters_per_token', None)\n",
    "    train_data = read_corpus(opt.train_path, token_embedder_max_chars, opt.max_sent_len)\n",
    "\n",
    "    logging.info('training instance: {}, training tokens: {}.'.format(len(train_data),\n",
    "                                                                        sum([len(s) - 1 for s in train_data])))\n",
    "\n",
    "    if opt.valid_path is not None:\n",
    "        valid_data = read_corpus(opt.valid_path, token_embedder_max_chars, opt.max_sent_len)\n",
    "        logging.info('valid instance: {}, valid tokens: {}.'.format(len(valid_data),\n",
    "                                                                    sum([len(s) - 1 for s in valid_data])))\n",
    "    elif opt.valid_size > 0:\n",
    "        train_data, valid_data = divide(train_data, opt.valid_size)\n",
    "        logging.info('training instance: {}, training tokens after division: {}.'.format(\n",
    "          len(train_data), sum([len(s) - 1 for s in train_data])))\n",
    "        logging.info('valid instance: {}, valid tokens: {}.'.format(\n",
    "          len(valid_data), sum([len(s) - 1 for s in valid_data])))\n",
    "    else:\n",
    "        valid_data = None\n",
    "\n",
    "    if opt.test_path is not None:\n",
    "        test_data = read_corpus(opt.test_path, token_embedder_max_chars, opt.max_sent_len)\n",
    "        logging.info('testing instance: {}, testing tokens: {}.'.format(\n",
    "          len(test_data), sum([len(s) - 1 for s in test_data])))\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    if opt.word_embedding is not None:\n",
    "        embs = load_embedding(opt.word_embedding)\n",
    "        word_lexicon = {word: i for i, word in enumerate(embs[0])}  \n",
    "    else:\n",
    "        embs = None\n",
    "        word_lexicon = {}\n",
    "\n",
    "    # Maintain the vocabulary. vocabulary is used in either WordEmbeddingInput or softmax classification\n",
    "    vocab = get_truncated_vocab(train_data, opt.min_count)\n",
    "\n",
    "    # Ensure index of '<oov>' is 0\n",
    "    for special_word in ['<oov>', '<bos>', '<eos>',  '<pad>']:\n",
    "        if special_word not in word_lexicon:\n",
    "            word_lexicon[special_word] = len(word_lexicon)\n",
    "\n",
    "    for word, _ in vocab:\n",
    "        if word not in word_lexicon:\n",
    "            word_lexicon[word] = len(word_lexicon)\n",
    "\n",
    "    # Word Embedding\n",
    "    if config['token_embedder']['word_dim'] > 0:\n",
    "        word_emb_layer = EmbeddingLayer(config['token_embedder']['word_dim'], word_lexicon, fix_emb=False, embs=embs)\n",
    "        logging.info('Word embedding size: {0}'.format(len(word_emb_layer.word2id)))\n",
    "    else:\n",
    "        word_emb_layer = None\n",
    "        logging.info('Vocabulary size: {0}'.format(len(word_lexicon)))\n",
    "\n",
    "    # Character Lexicon\n",
    "    if config['token_embedder']['char_dim'] > 0:\n",
    "        char_lexicon = {}\n",
    "        for sentence in train_data:\n",
    "            for word in sentence:\n",
    "                for ch in word:\n",
    "                    if ch not in char_lexicon:\n",
    "                        char_lexicon[ch] = len(char_lexicon)\n",
    "\n",
    "        for special_char in ['<bos>', '<eos>', '<oov>', '<pad>', '<bow>', '<eow>']:\n",
    "            if special_char not in char_lexicon:\n",
    "                char_lexicon[special_char] = len(char_lexicon)\n",
    "\n",
    "        char_emb_layer = EmbeddingLayer(config['token_embedder']['char_dim'], char_lexicon, fix_emb=False)\n",
    "        logging.info('Char embedding size: {0}'.format(len(char_emb_layer.word2id)))\n",
    "    else:\n",
    "        char_lexicon = None\n",
    "        char_emb_layer = None\n",
    "\n",
    "    train = create_batches(\n",
    "      train_data, opt.batch_size, word_lexicon, char_lexicon, config, use_cuda=use_cuda)\n",
    "\n",
    "    if opt.eval_steps is None:\n",
    "        opt.eval_steps = len(train[0])\n",
    "    logging.info('Evaluate every {0} batches.'.format(opt.eval_steps))\n",
    "\n",
    "    if valid_data is not None:\n",
    "        valid = create_batches(\n",
    "          valid_data, opt.batch_size, word_lexicon, char_lexicon, config, sort=False, shuffle=False, use_cuda=use_cuda)\n",
    "    else:\n",
    "        valid = None\n",
    "\n",
    "    if test_data is not None:\n",
    "        test = create_batches(\n",
    "          test_data, opt.batch_size, word_lexicon, char_lexicon, config, sort=False, shuffle=False, use_cuda=use_cuda)\n",
    "    else:\n",
    "        test = None\n",
    "\n",
    "    label_to_ix = word_lexicon\n",
    "    logging.info('vocab size: {0}'.format(len(label_to_ix)))\n",
    "\n",
    "    nclasses = len(label_to_ix)\n",
    "\n",
    "    model = Model(config, word_emb_layer, char_emb_layer, nclasses, use_cuda)\n",
    "    logging.info(str(model))\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    need_grad = lambda x: x.requires_grad\n",
    "    if opt.optimizer.lower() == 'adam':\n",
    "        optimizer = optim.Adam(filter(need_grad, model.parameters()), lr=opt.lr)\n",
    "    elif opt.optimizer.lower() == 'sgd':\n",
    "        optimizer = optim.SGD(filter(need_grad, model.parameters()), lr=opt.lr)\n",
    "    elif opt.optimizer.lower() == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter(need_grad, model.parameters()), lr=opt.lr)\n",
    "    else:\n",
    "        raise ValueError('Unknown optimizer {}'.format(opt.optimizer.lower()))\n",
    "\n",
    "    try:\n",
    "        os.makedirs(opt.model)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "    if config['token_embedder']['char_dim'] > 0:\n",
    "        with codecs.open(os.path.join(opt.model, 'char.dic'), 'w', encoding='utf-8') as fpo:\n",
    "            for ch, i in char_emb_layer.word2id.items():\n",
    "                print('{0}\\t{1}'.format(ch, i), file=fpo)\n",
    "\n",
    "    with codecs.open(os.path.join(opt.model, 'word.dic'), 'w', encoding='utf-8') as fpo:\n",
    "        for w, i in word_lexicon.items():\n",
    "            print('{0}\\t{1}'.format(w, i), file=fpo)\n",
    "\n",
    "    json.dump(vars(opt), codecs.open(os.path.join(opt.model, 'config.json'), 'w', encoding='utf-8'))\n",
    "\n",
    "    best_train = 1e+8\n",
    "    best_valid = 1e+8\n",
    "    test_result = 1e+8\n",
    "\n",
    "    for epoch in range(opt.max_epoch):\n",
    "        best_train, best_valid, test_result = train_model(epoch, opt, model, optimizer,\n",
    "                                                          train, valid, test, best_train, best_valid, test_result)\n",
    "        if opt.lr_decay > 0:\n",
    "            optimizer.param_groups[0]['lr'] *= opt.lr_decay\n",
    "\n",
    "    if valid_data is None:\n",
    "        logging.info(\"best train ppl: {:.6f}.\".format(best_train))\n",
    "    elif test_data is None:\n",
    "        logging.info(\"best train ppl: {:.6f}, best valid ppl: {:.6f}.\".format(best_train, best_valid))\n",
    "    else:\n",
    "        logging.info(\"best train ppl: {:.6f}, best valid ppl: {:.6f}, test ppl: {:.6f}.\".format(best_train, best_valid, test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    cmd = argparse.ArgumentParser('The testing components of')\n",
    "    cmd.add_argument('--gpu', default=-1, type=int, help='use id of gpu, -1 if cpu.')\n",
    "    cmd.add_argument(\"--input\", help=\"the path to the raw text file.\")\n",
    "    cmd.add_argument(\"--model\", required=True, help=\"path to save model\")\n",
    "    cmd.add_argument(\"--batch_size\", \"--batch\", type=int, default=1, help='the batch size.')\n",
    "    args = cmd.parse_args(sys.argv[2:])\n",
    "\n",
    "    if args.gpu >= 0:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "    use_cuda = args.gpu >= 0 and torch.cuda.is_available()\n",
    "\n",
    "    args2 = dict2namedtuple(json.load(codecs.open(os.path.join(args.model, 'config.json'), 'r', encoding='utf-8')))\n",
    "\n",
    "    with open(args2.config_path, 'r') as fin:\n",
    "        config = json.load(fin)\n",
    "\n",
    "    if config['token_embedder']['char_dim'] > 0:\n",
    "        char_lexicon = {}\n",
    "        with codecs.open(os.path.join(args.model, 'char.dic'), 'r', encoding='utf-8') as fpi:\n",
    "            for line in fpi:\n",
    "                tokens = line.strip().split('\\t')\n",
    "                if len(tokens) == 1:\n",
    "                    tokens.insert(0, '\\u3000')\n",
    "                token, i = tokens\n",
    "                char_lexicon[token] = int(i)\n",
    "        char_emb_layer = EmbeddingLayer(config['token_embedder']['char_dim'], char_lexicon, fix_emb=False)\n",
    "        logging.info('char embedding size: ' + str(len(char_emb_layer.word2id)))\n",
    "    else:\n",
    "        char_lexicon = None\n",
    "        char_emb_layer = None\n",
    "\n",
    "    word_lexicon = {}\n",
    "    with codecs.open(os.path.join(args.model, 'word.dic'), 'r', encoding='utf-8') as fpi:\n",
    "        for line in fpi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            if len(tokens) == 1:\n",
    "                tokens.insert(0, '\\u3000')\n",
    "            token, i = tokens\n",
    "            word_lexicon[token] = int(i)\n",
    "\n",
    "    if config['token_embedder']['word_dim'] > 0:\n",
    "        word_emb_layer = EmbeddingLayer(config['token_embedder']['word_dim'], word_lexicon, fix_emb=False, embs=None)\n",
    "        logging.info('word embedding size: ' + str(len(word_emb_layer.word2id)))\n",
    "    else:\n",
    "        word_emb_layer = None\n",
    "\n",
    "    model = Model(config, word_emb_layer, char_emb_layer, len(word_lexicon), use_cuda)\n",
    "\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    logging.info(str(model))\n",
    "    model.load_model(args.model)\n",
    "    test = read_corpus(args.input, config['token_embedder']['max_characters_per_token'], max_sent_len=10000)\n",
    "\n",
    "    test_w, test_c, test_lens, test_masks = create_batches(\n",
    "      test, args.batch_size, word_lexicon, char_lexicon, config, sort=False, shuffle=False, use_cuda=use_cuda)\n",
    "\n",
    "    test_result = eval_model(model, (test_w, test_c, test_lens, test_masks))\n",
    "\n",
    "    logging.info(\"test_ppl={:.6f}\".format(test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

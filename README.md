# korean-embedding-study
이기창(ratsgo)님의 자연어 처리 저서 '한국어 임베딩' 스터디 기록 저장소입니다. 모든 구현은 PyTorch를 이용해 이루어질 예정입니다.
<br/>

## Study sessions
- **Week 1.** Chapter 1. 서론 - Chapter 3. 한국어 전처리
	- 1장. 서론 (by. 종춘)
	- 2장. 벡터가 어떻게 의미를 가지게 되는가 (by. 수민)
	- 3장. 한국어 전처리 (by. 훈)

- **Week 2.** Chapter 4. 단어 수준 임베딩
	- 4.1 NPLM ~ 4.3 FastText (by. 민수)
	- 4.4 잠재 의미 분석 ~ 4.8 가중 임베딩 (by. 혜리)

- **Week 3.** Chapter 5. 문장 수준 임베딩
	- 5.1 잠재 의미 분석 ~ 5.4 ELMo (by. OO)
	- 5.5 트랜스포머 네트워크 with Transformer (Attention Is All You Need) paper review (by. OO)

- **Week 4.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- Transformer 코드 리뷰 **with. basic implementation** (by. 훈)

- **Week 5.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- ELMo (Deep contextualized word representations) paper review (by. OO)
	- ELMo 코드 리뷰 **with. AllenNLP** (by. OO)

- **Week 6.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- ELMo 코드 리뷰 **with. basic implementation** (by. OO)

- **Week 7.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- 5.6 BERT with BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding) paper review (by. OO)
	- BERT 코드 리뷰 **with. hugging face** (by. OO)

- **Week 8.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- BERT 코드 리뷰 **with. basic implementation** (by. OO)

- **Week 9.** Chapter 6. 임베딩 파인 튜닝
	- 6.1 프리트레인과 파인 튜닝 ~ 6.6 어떤 문장 임베딩을 사용할 것인가 (by. OO)

- **Week 10.** 논문 리뷰 세션
	- [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (by. OO)
	- [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450) (by.OO)


<br/>

## Libraries
- [AllenNLP](https://github.com/allenai/allennlp)
- [hugging face's transformers](https://github.com/huggingface/transformers)
- [sentencepiece](https://github.com/google/sentencepiece)
- [soynlp](https://github.com/lovit/soynlp)


<br/>

## Related Papers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450)
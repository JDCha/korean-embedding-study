# 이기창 저 '한국어 임베딩' 스터디
이기창(ratsgo)님의 자연어 처리 저서 '한국어 임베딩' 스터디 기록 저장소입니다. 

모든 구현 및 실습은 PyTorch를 이용해 이루어질 예정입니다.

---

<br/>

## People
[허 훈](https://github.com/Huffon), [이인환](https://github.com/lih0905), 박병준, [정민수](https://github.com/4seaday), [정지용](https://github.com/jistim), 배종춘, [이혜리](https://github.com/keirahrlee), [한수민](https://github.com/dlems), Onur Sahil

<br/>


## Study sessions
- **Week 1.** Chapter 1. 서론 - Chapter 3. 한국어 전처리
	- 1장. 서론 (by. 수민)
	- 2장. 벡터가 어떻게 의미를 가지게 되는가 (by. 종춘)
	- 3장. 한국어 전처리 (by. 훈)

- **Week 2.** Chapter 4. 단어 수준 임베딩
	- 4.1 NPLM ~ 4.4 잠재 의미 분석 (by. 민수)

- **Week 3.** Chapter 4. 단어 수준 임베딩 (cont'd)
	- 4.5 GloVe ~ 4.8 가중 임베딩 (by. 혜리)

- **Week 4.** Chapter 5. 문장 수준 임베딩
	- 5.1 잠재 의미 분석 ~ 5.4 ELMo (by. 지용)

---

- **Week 5.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- 5.5 트랜스포머 네트워크 with Transformer (Attention Is All You Need) paper review (by. 훈)
	- Transformer 코드 리뷰 **with. basic implementation** (by. 인환)

- **Week 6.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- ELMo (Deep contextualized word representations) paper review (by. 병준)
	- ELMo 코드 리뷰 **with. AllenNLP** (by. 민수)

- **Week 7.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- ELMo 코드 리뷰 **with. basic implementation** (by. 지용)

- **Week 8.** Chapter 5. 문장 수준 임베딩 (cont'd)
	- 5.6 BERT with BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding) paper review (by. 혜리)
	- BERT 코드 리뷰 **with. basic implementation** (by. 훈) : [**_Link_**](https://github.com/Huffon/pytorch-bert)

<br/>

## Repositories
- [AllenNLP](https://github.com/allenai/allennlp)
- [hugging face's transformers](https://github.com/huggingface/transformers)
- [sentencepiece](https://github.com/google/sentencepiece)
- [soynlp](https://github.com/lovit/soynlp)
- [korean-embedding](https://github.com/ratsgo/embedding)


<br/>

## Related Papers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450)
